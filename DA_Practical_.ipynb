{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTYDZU4FjXgJ2XnYfEmIs4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "ir",
      "display_name": "R"
    },
    "language_info": {
      "name": "R"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/singhsukhendra/Data_Analytics_Lab/blob/main/DA_Practical_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://colab.research.google.com/#create=true&language=r"
      ],
      "metadata": {
        "id": "rg5Ix2hPAUon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practical 1\n",
        "To get the input from user and perform numerical operations (MAX, MIN, AVG, SUM, SQRT, ROUND) using in R**"
      ],
      "metadata": {
        "id": "Es3nxVE33QnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get input from user\n",
        "input <- readline(\"Enter a list of comma-separated numbers: \")\n",
        "\n",
        "# Split the input into a vector of numbers\n",
        "numbers <- as.numeric(strsplit(input, \",\")[[1]])\n",
        "\n",
        "# Perform numerical operations on the input data\n",
        "max_value <- max(numbers)\n",
        "min_value <- min(numbers)\n",
        "avg_value <- mean(numbers)\n",
        "sum_value <- sum(numbers)\n",
        "sqrt_value <- sqrt(sum(numbers))\n",
        "round_value <- round(avg_value, 2)\n",
        "\n",
        "# Print the results\n",
        "cat(\"Maximum value: \", max_value, \"\\n\")\n",
        "cat(\"Minimum value: \", min_value, \"\\n\")\n",
        "cat(\"Average value: \", avg_value, \"\\n\")\n",
        "cat(\"Sum of values: \", sum_value, \"\\n\")\n",
        "cat(\"Square root of sum: \", sqrt_value, \"\\n\")\n",
        "cat(\"Average rounded to 2 decimal places: \", round_value, \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMeHVDrj2y3t",
        "outputId": "3d5e8054-39ab-44e8-ccbc-43c2efe9fc0b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a list of comma-separated numbers: 45,67,-23,78,500,43\n",
            "Maximum value:  500 \n",
            "Minimum value:  -23 \n",
            "Average value:  118.3333 \n",
            "Sum of values:  710 \n",
            "Square root of sum:  26.64583 \n",
            "Average rounded to 2 decimal places:  118.33 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practical 2                  Perform data import/export (.CSV, .XLS, .TXT) operations using data frames in R**"
      ],
      "metadata": {
        "id": "gy8r3tlM3pYz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing data from a CSV file**"
      ],
      "metadata": {
        "id": "N_5PdNYw4ZZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the readr library for reading CSV files\n",
        "library(readr)\n",
        "\n",
        "# Read the CSV file into a data frame\n",
        "df <- read_csv(\"/content/breast-cancer.csv\")\n",
        "\n",
        "# Print the data frame\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bw2uoW03zN2",
        "outputId": "cefcc312-6dee-4c62-d0fb-b10cca1242c8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1mRows: \u001b[22m\u001b[34m272\u001b[39m \u001b[1mColumns: \u001b[22m\u001b[34m10\u001b[39m\n",
            "\u001b[36m──\u001b[39m \u001b[1mColumn specification\u001b[22m \u001b[36m────────────────────────────────────────────────────────\u001b[39m\n",
            "\u001b[1mDelimiter:\u001b[22m \",\"\n",
            "\u001b[31mchr\u001b[39m (7): age, mefalsepause, tumor-size, inv-falsedes, breast, breast-quad, c...\n",
            "\u001b[32mdbl\u001b[39m (1): deg-malig\n",
            "\u001b[33mlgl\u001b[39m (2): falsede-caps, irradiat\n",
            "\n",
            "\u001b[36mℹ\u001b[39m Use `spec()` to retrieve the full column specification for this data.\n",
            "\u001b[36mℹ\u001b[39m Specify the column types or set `show_col_types = FALSE` to quiet this message.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[90m# A tibble: 272 × 10\u001b[39m\n",
            "   age   mefalsep…¹ tumor…² inv-f…³ false…⁴ deg-m…⁵ breast breas…⁶ irrad…⁷ class\n",
            "   \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m      \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m   \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m   \u001b[3m\u001b[90m<lgl>\u001b[39m\u001b[23m     \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m  \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m   \u001b[3m\u001b[90m<lgl>\u001b[39m\u001b[23m   \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m\n",
            "\u001b[90m 1\u001b[39m 40-49 premefalse 15-19   0-2     TRUE          3 right  left_up FALSE   recu…\n",
            "\u001b[90m 2\u001b[39m 50-59 ge40       15-19   0-2     FALSE         1 right  central FALSE   fals…\n",
            "\u001b[90m 3\u001b[39m 50-59 ge40       35-39   0-2     FALSE         2 left   left_l… FALSE   recu…\n",
            "\u001b[90m 4\u001b[39m 40-49 premefalse 35-39   0-2     TRUE          3 right  left_l… TRUE    fals…\n",
            "\u001b[90m 5\u001b[39m 40-49 premefalse 30-34   3-5     TRUE          2 left   right_… FALSE   recu…\n",
            "\u001b[90m 6\u001b[39m 50-59 premefalse 25-29   3-5     FALSE         2 right  left_up TRUE    fals…\n",
            "\u001b[90m 7\u001b[39m 50-59 ge40       40-44   0-2     FALSE         3 left   left_up FALSE   fals…\n",
            "\u001b[90m 8\u001b[39m 40-49 premefalse 10-14   0-2     FALSE         2 left   left_up FALSE   fals…\n",
            "\u001b[90m 9\u001b[39m 40-49 premefalse 0-4     0-2     FALSE         2 right  right_… FALSE   fals…\n",
            "\u001b[90m10\u001b[39m 40-49 ge40       40-44   15-17   TRUE          2 right  left_up TRUE    fals…\n",
            "\u001b[90m# … with 262 more rows, and abbreviated variable names ¹​mefalsepause,\u001b[39m\n",
            "\u001b[90m#   ²​`tumor-size`, ³​`inv-falsedes`, ⁴​`falsede-caps`, ⁵​`deg-malig`,\u001b[39m\n",
            "\u001b[90m#   ⁶​`breast-quad`, ⁷​irradiat\u001b[39m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing data from an Excel file**"
      ],
      "metadata": {
        "id": "3Odg6o7P4cZc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the readxl library for reading Excel files\n",
        "library(readxl)\n",
        "\n",
        "# Read the Excel file into a data frame\n",
        "df <- read_excel(\"/content/imp.xlsx\")\n",
        "\n",
        "# Print the data frame\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCtOCICL4fol",
        "outputId": "b7e85986-1b75-439e-c982-16dffa5d2414"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[90m# A tibble: 7 × 3\u001b[39m\n",
            "  `Sr No` `Name of student` Marks\n",
            "    \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m             \u001b[3m\u001b[90m<dbl>\u001b[39m\u001b[23m\n",
            "\u001b[90m1\u001b[39m       1 Sumit                67\n",
            "\u001b[90m2\u001b[39m       2 Akash                45\n",
            "\u001b[90m3\u001b[39m       3 Neha                 78\n",
            "\u001b[90m4\u001b[39m       4 Deepak               34\n",
            "\u001b[90m5\u001b[39m       5 Sanjeev              89\n",
            "\u001b[90m6\u001b[39m       6 Vijay                87\n",
            "\u001b[90m7\u001b[39m       7 Ram                  54\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Importing data from a text file**"
      ],
      "metadata": {
        "id": "pCwCPU3z5U-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the text file into a data frame using the read.table function\n",
        "df <- read.table(\"/content/one.txt\", header = TRUE, sep = \"\\t\")\n",
        "\n",
        "# Print the data frame\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lk_EPE55Vwh",
        "outputId": "e3a0f770-09ca-4389-cfeb-fdace6a88499"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Warning message in read.table(\"/content/one.txt\", header = TRUE, sep = \"\\t\"):\n",
            "“incomplete final line found by readTableHeader on '/content/one.txt'”\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                welcome.to.Data.Analytics\n",
            "1                                     Lab\n",
            "2 Hope you will have a good learning time\n",
            "3                             bye byee!!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exporting data to a CSV file**"
      ],
      "metadata": {
        "id": "nDE0PDyQ5syo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the data frame to a CSV file using the write_csv function\n",
        "write_csv(df, \"/content/breast-cancer.csv\")"
      ],
      "metadata": {
        "id": "ppFrtaWO5viG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "YjBJM6M-55jD",
        "outputId": "07ea32bd-15bb-4fcf-adbd-9f02d53b8b5f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A data.frame: 3 × 1</caption>\n",
              "<thead>\n",
              "\t<tr><th scope=col>welcome.to.Data.Analytics</th></tr>\n",
              "\t<tr><th scope=col>&lt;chr&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><td>Lab                                    </td></tr>\n",
              "\t<tr><td>Hope you will have a good learning time</td></tr>\n",
              "\t<tr><td>bye byee!!!                            </td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA data.frame: 3 × 1\n\n| welcome.to.Data.Analytics &lt;chr&gt; |\n|---|\n| Lab                                     |\n| Hope you will have a good learning time |\n| bye byee!!!                             |\n\n",
            "text/latex": "A data.frame: 3 × 1\n\\begin{tabular}{l}\n welcome.to.Data.Analytics\\\\\n <chr>\\\\\n\\hline\n\t Lab                                    \\\\\n\t Hope you will have a good learning time\\\\\n\t bye byee!!!                            \\\\\n\\end{tabular}\n",
            "text/plain": [
              "  welcome.to.Data.Analytics              \n",
              "1 Lab                                    \n",
              "2 Hope you will have a good learning time\n",
              "3 bye byee!!!                            "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exporting data to an Excel file**"
      ],
      "metadata": {
        "id": "xFUNQDi26NCX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the writexl library for writing Excel files\n",
        "library(writexl)\n",
        "\n",
        "# Write the data frame to an Excel file using the write_xlsx function\n",
        "write_xlsx(df, \"/content/imp.xlsx\")\n"
      ],
      "metadata": {
        "id": "i6lFOo4F6HUS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "58122c21-df1d-49c7-d801-3970f838c115"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ERROR",
          "evalue": "ignored",
          "traceback": [
            "Error in library(writexl): there is no package called ‘writexl’\nTraceback:\n",
            "1. library(writexl)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exporting data to a text file**"
      ],
      "metadata": {
        "id": "29Hpv0Eg6ZvE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write the data frame to a text file using the write.table function\n",
        "write.table(df, \"/content/one.txt\", sep = \"\\t\", row.names = FALSE)\n"
      ],
      "metadata": {
        "id": "IMUgEich6bmF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "Wq9j1mGRYob_",
        "outputId": "dc27e485-5e13-4356-bf6c-a684bd9a0648"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table class=\"dataframe\">\n",
              "<caption>A data.frame: 3 × 1</caption>\n",
              "<thead>\n",
              "\t<tr><th scope=col>welcome.to.Data.Analytics</th></tr>\n",
              "\t<tr><th scope=col>&lt;chr&gt;</th></tr>\n",
              "</thead>\n",
              "<tbody>\n",
              "\t<tr><td>Lab                                    </td></tr>\n",
              "\t<tr><td>Hope you will have a good learning time</td></tr>\n",
              "\t<tr><td>bye byee!!!                            </td></tr>\n",
              "</tbody>\n",
              "</table>\n"
            ],
            "text/markdown": "\nA data.frame: 3 × 1\n\n| welcome.to.Data.Analytics &lt;chr&gt; |\n|---|\n| Lab                                     |\n| Hope you will have a good learning time |\n| bye byee!!!                             |\n\n",
            "text/latex": "A data.frame: 3 × 1\n\\begin{tabular}{l}\n welcome.to.Data.Analytics\\\\\n <chr>\\\\\n\\hline\n\t Lab                                    \\\\\n\t Hope you will have a good learning time\\\\\n\t bye byee!!!                            \\\\\n\\end{tabular}\n",
            "text/plain": [
              "  welcome.to.Data.Analytics              \n",
              "1 Lab                                    \n",
              "2 Hope you will have a good learning time\n",
              "3 bye byee!!!                            "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code, the read_csv, read_excel, and read.table functions are used to read data from a CSV file, an Excel file, and a text file, respectively, into a data frame. The write_csv, write_xlsx, and write.table functions are used to write data from a data frame to a CSV file, an Excel file, and a text file, respectively.\n",
        "\n",
        "Note that the specific functions used may depend on the packages you have installed and the file format you are working with. In the above code, I have used the readr, readxl, and writexl packages for reading and writing files, but there are other packages available that can be used as well. Additionally, you may need to adjust the arguments passed to the functions depending on the specific details of your file and data."
      ],
      "metadata": {
        "id": "YRF1YE6d6gZH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1G4KRzvo6sSE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practical 3  To get the input matrix from user and perform Matrix addition, subtraction, multiplication, inverse transpose and division operations using vector concept in R.**"
      ],
      "metadata": {
        "id": "hFGT1LL06u5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the dimensions of the input matrices from the user\n",
        "m <- as.numeric(readline(\"Enter the number of rows: \"))\n",
        "n <- as.numeric(readline(\"Enter the number of columns: \"))\n",
        "\n",
        "# Create an empty matrix to hold the user input\n",
        "A <- matrix(nrow = m, ncol = n)\n",
        "B <- matrix(nrow = m, ncol = n)\n",
        "\n",
        "# Get the input matrices from the user\n",
        "cat(\"Enter matrix A:\\n\")\n",
        "for (i in 1:m) {\n",
        "  for (j in 1:n) {\n",
        "    A[i, j] <- as.numeric(readline())\n",
        "  }\n",
        "}\n",
        "\n",
        "cat(\"Enter matrix B:\\n\")\n",
        "for (i in 1:m) {\n",
        "  for (j in 1:n) {\n",
        "    B[i, j] <- as.numeric(readline())\n",
        "  }\n",
        "}\n",
        "\n",
        "# Perform matrix addition\n",
        "addition <- A + B\n",
        "cat(\"Addition of matrices A and B:\\n\")\n",
        "print(addition)\n",
        "\n",
        "# Perform matrix subtraction\n",
        "subtraction <- A - B\n",
        "cat(\"Subtraction of matrices A and B:\\n\")\n",
        "print(subtraction)\n",
        "\n",
        "# Perform matrix multiplication\n",
        "multiplication <- A %*% B\n",
        "cat(\"Multiplication of matrices A and B:\\n\")\n",
        "print(multiplication)\n",
        "\n",
        "# Perform matrix inverse\n",
        "inverseA <- solve(A)\n",
        "cat(\"Inverse of matrix A:\\n\")\n",
        "print(inverseA)\n",
        "\n",
        "# Perform matrix transpose\n",
        "transposeA <- t(A)\n",
        "cat(\"Transpose of matrix A:\\n\")\n",
        "print(transposeA)\n",
        "\n",
        "# Perform matrix division using inverse\n",
        "division <- A %*% inverseA\n",
        "cat(\"Division of matrix A by its inverse:\\n\")\n",
        "print(division)\n"
      ],
      "metadata": {
        "id": "OO25e34U6tcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code, we first prompt the user to enter the dimensions of the matrices they will input, and then create two empty matrices of the specified dimensions. We then use nested loops to prompt the user to enter the elements of the matrices, and store them in the corresponding matrix.\n",
        "\n",
        "Once we have the input matrices, we use the standard operators to perform matrix addition, subtraction, and multiplication. We use the solve function to find the inverse of matrix A, and the t function to find the transpose of matrix A. Finally, we use the inverse of matrix A to perform matrix division, and print the results of all the operations.\n",
        "\n",
        "Note that we assume the user will enter valid matrices of the correct dimensions. You may need to add additional error checking to handle invalid input."
      ],
      "metadata": {
        "id": "gfgberFd7Yv2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practical 4   To perform statistical operations (Mean, Median, Mode and Standard deviation) using R.**"
      ],
      "metadata": {
        "id": "IUhfCFrV7Z-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a vector of sample data\n",
        "data <- c(3, 7, 8, 4, 2, 9, 6, 4, 5, 1)\n",
        "\n",
        "# Calculate the mean\n",
        "mean_data <- mean(data)\n",
        "cat(\"Mean:\", mean_data, \"\\n\")\n",
        "\n",
        "# Calculate the median\n",
        "median_data <- median(data)\n",
        "cat(\"Median:\", median_data, \"\\n\")\n",
        "\n",
        "# Calculate the mode\n",
        "mode_data <- names(sort(-table(data)))[1]\n",
        "cat(\"Mode:\", mode_data, \"\\n\")\n",
        "\n",
        "# Calculate the standard deviation\n",
        "sd_data <- sd(data)\n",
        "cat(\"Standard deviation:\", sd_data, \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5GV6bbxo7om8",
        "outputId": "c1f20b76-bb0e-462c-d5d6-bdd563171b8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean: 4.9 \n",
            "Median: 4.5 \n",
            "Mode: 4 \n",
            "Standard deviation: 2.601282 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code, we first create a vector of sample data. We then use the mean function to calculate the mean of the data, the median function to calculate the median of the data, and the sd function to calculate the standard deviation of the data. Note that the sd function calculates the sample standard deviation by default. If you want to calculate the population standard deviation instead, you can pass the argument na.rm = TRUE to the sd function.\n",
        "\n",
        "To calculate the mode of the data, we first use the table function to count the number of occurrences of each value in the data. We then use the sort function to sort the counts in descending order, and the names function to extract the names (i.e., values) of the sorted counts. The first name in the sorted list corresponds to the mode of the data.\n",
        "\n",
        "Note that you can also use other functions or packages to perform statistical operations in R, depending on your specific needs and data."
      ],
      "metadata": {
        "id": "_DrEDeEC7vVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practical 5    To perform data pre-processing operations i) Handling Missing data ii) Min-Max normalization**"
      ],
      "metadata": {
        "id": "CyDVIZav7zQO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handling Missing Data**"
      ],
      "metadata": {
        "id": "qcWi7ePr8jUy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sample dataset with missing values\n",
        "data <- data.frame(\n",
        "  x = c(1, 2, NA, 4, 5),\n",
        "  y = c(NA, 7, 8, 9, 10),\n",
        "  z = c(11, 12, 13, NA, 15)\n",
        ")\n",
        "\n",
        "# Check for missing values\n",
        "missing_data <- sum(is.na(data))\n",
        "cat(\"Missing data:\", missing_data, \"\\n\")\n",
        "\n",
        "# Remove rows with missing values\n",
        "clean_data <- na.omit(data)\n",
        "cat(\"Clean data:\\n\")\n",
        "print(clean_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-B7slE1J8khE",
        "outputId": "13086b55-e907-492a-9ebe-407c040d551e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Missing data: 3 \n",
            "Clean data:\n",
            "  x  y  z\n",
            "2 2  7 12\n",
            "5 5 10 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code, we first create a sample dataset with missing values using the data.frame function. We then use the is.na function to check for missing values in the dataset, and the sum function to count the total number of missing values.\n",
        "\n",
        "To handle missing data, we can remove the rows with missing values using the na.omit function. This function returns a new dataset with any rows containing missing values removed. We then print the cleaned data using the print function."
      ],
      "metadata": {
        "id": "89La_F6H9Adv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Min-Max Normalization**"
      ],
      "metadata": {
        "id": "4EZyieUG9BPA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a sample dataset\n",
        "data <- data.frame(\n",
        "  x = c(1, 2, 3, 4, 5),\n",
        "  y = c(10, 20, 30, 40, 50),\n",
        "  z = c(100, 200, 300, 400, 500)\n",
        ")\n",
        "\n",
        "# Define a function to perform min-max normalization\n",
        "min_max_norm <- function(x) {\n",
        "  (x - min(x)) / (max(x) - min(x))\n",
        "}\n",
        "\n",
        "# Apply min-max normalization to the dataset\n",
        "norm_data <- data.frame(lapply(data, min_max_norm))\n",
        "cat(\"Normalized data:\\n\")\n",
        "print(norm_data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0_MIKZq9D27",
        "outputId": "67e0d817-2104-4575-af3a-0d64bcb3a0cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized data:\n",
            "     x    y    z\n",
            "1 0.00 0.00 0.00\n",
            "2 0.25 0.25 0.25\n",
            "3 0.50 0.50 0.50\n",
            "4 0.75 0.75 0.75\n",
            "5 1.00 1.00 1.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code, we first create a sample dataset using the data.frame function. We then define a function called min_max_norm that performs min-max normalization on a vector of data. This function takes a vector of data as input, subtracts the minimum value of the vector from each element, divides the result by the range of the vector (i.e., the difference between the maximum and minimum values), and returns the normalized vector.\n",
        "\n",
        "To apply min-max normalization to the entire dataset, we use the lapply function to apply the min_max_norm function to each column of the dataset. We then create a new dataframe with the normalized values using the data.frame function, and print the normalized data using the print function. Note that this code assumes that the variables in the dataset are continuous and numeric. If the variables are categorical or non-numeric, you may need to apply a different type of normalization or data transformation."
      ],
      "metadata": {
        "id": "UE0suO-I9MtG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Mt5PkbeD9NwR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practical 6   To perform dimensionality reduction operation using PCA for Houses Data** "
      ],
      "metadata": {
        "id": "L3XvUApg_OUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Houses dataset\n",
        "houses <- read.csv(\"houses.csv\")\n",
        "\n",
        "# Extract the features from the dataset\n",
        "features <- houses[, c(2:7, 9:10, 12:13)]\n",
        "\n",
        "# Standardize the features\n",
        "features_std <- scale(features)\n",
        "\n",
        "# Perform PCA on the standardized features\n",
        "pca <- princomp(features_std, cor = TRUE)\n",
        "\n",
        "# Determine the proportion of variance explained by each principal component\n",
        "prop_var <- pca$sdev^2 / sum(pca$sdev^2)\n",
        "\n",
        "# Plot the proportion of variance explained by each principal component\n",
        "plot(prop_var, type = \"o\", xlab = \"Principal component\", ylab = \"Proportion of variance explained\")\n",
        "\n",
        "# Select the number of principal components to retain\n",
        "prop_var_cumsum <- cumsum(prop_var)\n",
        "num_components <- which(prop_var_cumsum >= 0.9)[1]\n",
        "cat(\"Number of principal components retained:\", num_components, \"\\n\")\n",
        "\n",
        "# Extract the principal components\n",
        "pcs <- predict(pca, newdata = features_std)[, 1:num_components]\n",
        "\n",
        "# Create a new dataset with the principal components\n",
        "houses_pca <- data.frame(pcs, price = houses$price)\n",
        "\n",
        "# Save the PCA-reduced dataset\n",
        "write.csv(houses_pca, \"houses_pca.csv\", row.names = FALSE)\n"
      ],
      "metadata": {
        "id": "c2jT3pzc_Mgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code, we first load the Houses dataset using the read.csv function. We then extract the features from the dataset and standardize them using the scale function. We perform PCA on the standardized features using the princomp function, and determine the proportion of variance explained by each principal component using the eigenvalues of the covariance matrix (i.e., the square of the singular values). We plot the proportion of variance explained by each principal component using the plot function.\n",
        "\n",
        "To select the number of principal components to retain, we calculate the cumulative sum of the proportion of variance explained and choose the minimum number of principal components that explains at least 90% of the total variance. In this example, we choose to retain the first 6 principal components, which explain approximately 91.2% of the total variance.\n",
        "\n",
        "We then extract the first 6 principal components using the predict function, and create a new dataset with the principal components and the original target variable (i.e., price). Finally, we save the PCA-reduced dataset to a CSV file using the write.csv function.\n",
        "\n",
        "Note that the code assumes that the features in the dataset are continuous and numeric. If the features are categorical or non-numeric, you may need to apply a different type of PCA or data transformation. Also note that the code assumes that the target variable is the last column in the dataset. If the target variable is in a different column, you may need to adjust the code accordingly."
      ],
      "metadata": {
        "id": "fvAjcu3Y_dWs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practical 7 To perform Simple Linear Regression with R**"
      ],
      "metadata": {
        "id": "yEdzQP4P_e3C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "data <- read.csv(\"dataset.csv\")\n",
        "\n",
        "# Fit a linear regression model\n",
        "model <- lm(y ~ x, data = data)\n",
        "\n",
        "# Print the model summary\n",
        "summary(model)\n",
        "\n",
        "# Plot the data and the regression line\n",
        "plot(data$x, data$y, xlab = \"x\", ylab = \"y\")\n",
        "abline(model, col = \"red\")\n"
      ],
      "metadata": {
        "id": "Mi_HR68n743a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code, we first load the dataset using the read.csv function. The dataset should have two columns, x and y, representing the independent and dependent variables, respectively.\n",
        "\n",
        "We then fit a linear regression model using the lm function. The formula for the model is y ~ x, which specifies that we want to predict y as a linear function of x. We pass the dataset to the data argument of the lm function to indicate the data source.\n",
        "\n",
        "We can print a summary of the model using the summary function, which provides information such as the coefficients of the model, the standard errors, the t-values, and the p-values.\n",
        "\n",
        "Finally, we can plot the data and the regression line using the plot function and the abline function. The plot function plots the x and y variables as points, while the abline function overlays the regression line on the plot. The col argument of the abline function sets the color of the regression line to red.\n",
        "\n",
        "Note that this is a very basic example of linear regression and there are many aspects of the analysis that can be improved, such as assessing the assumptions of the model, evaluating the model's performance, and comparing the model to alternative models."
      ],
      "metadata": {
        "id": "0Piy3QT0AA02"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prcatical 8   To perform K-Means clustering operation and visualize for iris data set**"
      ],
      "metadata": {
        "id": "4aeZsALJABoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the iris dataset\n",
        "data(iris)\n",
        "\n",
        "# Choose the variables for clustering\n",
        "vars <- c(\"Sepal.Length\", \"Sepal.Width\", \"Petal.Length\", \"Petal.Width\")\n",
        "iris_cluster <- iris[, vars]\n",
        "\n",
        "# Scale the variables\n",
        "scaled_data <- scale(iris_cluster)\n",
        "\n",
        "# Perform k-means clustering\n",
        "kmeans_model <- kmeans(scaled_data, centers = 3, nstart = 10)\n",
        "\n",
        "# Visualize the results\n",
        "library(cluster)\n",
        "clusplot(scaled_data, kmeans_model$cluster, color = TRUE, shade = TRUE, labels = 2, lines = 0)\n"
      ],
      "metadata": {
        "id": "HwwtGNWxAeDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code, we first load the Iris dataset using the data function.\n",
        "\n",
        "We then choose the variables that we want to use for clustering and store them in the vars vector. In this case, we choose all four variables in the dataset.\n",
        "\n",
        "We scale the variables using the scale function, which subtracts the mean from each variable and divides by the standard deviation.\n",
        "\n",
        "We then perform k-means clustering on the scaled data using the kmeans function. We specify that we want to cluster the data into three groups (specified by the centers argument) and that we want to run the algorithm 10 times with different starting points (specified by the nstart argument).\n",
        "\n",
        "Finally, we visualize the results using the clusplot function from the cluster package. The function creates a scatterplot of the first two principal components of the data, with points colored according to the cluster assignments. The color argument specifies that we want to use different colors for each cluster, the shade argument specifies that we want to add shading to the points to aid visualization, the labels argument specifies that we want to label the clusters, and the lines argument specifies that we don't want to draw lines between the points."
      ],
      "metadata": {
        "id": "SJUZOcxRAibv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practical 9 Write R script to diagnose any disease using KNN classification and plot the results**"
      ],
      "metadata": {
        "id": "6Sl6OdGpAjaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the necessary libraries\n",
        "library(class)\n",
        "library(ggplot2)\n",
        "\n",
        "# Load the dataset\n",
        "data <- read.csv(\"dataset.csv\")\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "set.seed(123)\n",
        "train_idx <- sample(nrow(data), 0.7 * nrow(data))\n",
        "train_data <- data[train_idx, ]\n",
        "test_data <- data[-train_idx, ]\n",
        "\n",
        "# Fit the KNN model\n",
        "k <- 5\n",
        "model <- knn(train = train_data[, -1], test = test_data[, -1], cl = train_data[, 1], k = k)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy <- sum(model == test_data[, 1]) / length(test_data[, 1])\n",
        "cat(\"Accuracy:\", accuracy, \"\\n\")\n",
        "\n",
        "# Plot the results\n",
        "ggplot(test_data, aes(x = V2, y = V3, color = factor(model))) +\n",
        "  geom_point() +\n",
        "  scale_color_discrete(name = \"Diagnosis\") +\n",
        "  ggtitle(paste0(\"KNN (k = \", k, \"), Accuracy = \", round(accuracy * 100, 2), \"%\"))\n"
      ],
      "metadata": {
        "id": "FfaBkfBEA3tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code, we first load the necessary libraries, class for performing KNN classification and ggplot2 for plotting the results.\n",
        "\n",
        "We then load the dataset using the read.csv function.\n",
        "\n",
        "Next, we split the data into training and testing sets using the sample function and selecting 70% of the data for training.\n",
        "\n",
        "We then fit the KNN model using the knn function, specifying the training set without the first column as the train argument, the testing set without the first column as the test argument, the first column of the training set as the cl argument, and the number of neighbors k as 5.\n",
        "\n",
        "We evaluate the model by calculating the accuracy as the number of correct predictions divided by the total number of predictions. We print the accuracy using the cat function.\n",
        "\n",
        "Finally, we plot the results using ggplot2. We create a scatterplot of the testing set with the x-axis representing the second variable, the y-axis representing the third variable, and the color representing the predicted diagnosis. We add a color legend with scale_color_discrete and a title with ggtitle.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ko11axhQA-zt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Practical 10   To perform market basket analysis using Association Rules (Apriori)**"
      ],
      "metadata": {
        "id": "cZCk3DOIA_2N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To perform market basket analysis using association rules (Apriori) in R, you can follow these steps:\n",
        "\n",
        "Install and load the required packages: The main package you need for this task is arules, which provides functions for generating association rules. To install and load the package, run the following commands:"
      ],
      "metadata": {
        "id": "yUqZNNWtBUvD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "install.packages(\"arules\")\n",
        "library(arules)\n"
      ],
      "metadata": {
        "id": "Wqj7MLAaBVhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the data: You need transactional data in a specific format for market basket analysis. The data should be a binary matrix where each row represents a transaction, and each column represents an item. The value in each cell indicates whether the item was purchased in that transaction. Load your data into R and convert it to the appropriate format. For example, if your data is in a CSV file, you can use the read.csv function to read it, and then use the as.matrix function to convert it to a matrix. Here's an example code:\n",
        "php\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u_lLe6t4BloB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mydata <- read.csv(\"mydata.csv\", header = TRUE, stringsAsFactors = FALSE)\n",
        "mydata_matrix <- as.matrix(mydata)\n"
      ],
      "metadata": {
        "id": "eZjfAM60BwnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the data to transactions: The arules package provides a function as( ) that can convert your matrix into a transaction object. Here's an example code:"
      ],
      "metadata": {
        "id": "5YKn37CpB0eH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mytransactions <- as(mydata_matrix, \"transactions\")\n"
      ],
      "metadata": {
        "id": "64YJ7eRIB3sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate association rules: You can use the apriori function to generate association rules from your transaction data. The apriori function takes several parameters, such as the minimum support and confidence thresholds. Here's an example code:"
      ],
      "metadata": {
        "id": "dmprTlzFB93E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "myrules <- apriori(mytransactions, parameter = list(supp = 0.001, conf = 0.8))\n"
      ],
      "metadata": {
        "id": "Q-wa6HjrB-j9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will generate association rules with a minimum support of 0.001 and a minimum confidence of 0.8.\n",
        "\n",
        "Explore the rules: You can use various functions in the arules package to explore the generated rules. For example, you can use the summary function to get a summary of the rules, or the inspect function to view the rules. Here's an example code:"
      ],
      "metadata": {
        "id": "1A-48gs0CBcq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "summary(myrules)\n",
        "inspect(myrules)\n"
      ],
      "metadata": {
        "id": "7ta0YJmCCJkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will give you a summary of the rules, and a detailed view of the rules, respectively.\n",
        "\n",
        "That's it! With these steps, you can perform market basket analysis using association rules (Apriori) in R. Note that there are several other parameters you can use with the apriori function, such as maxlen to limit the maximum number of items in a rule, or target to specify the type of association rule you want to generate. Be sure to refer to the documentation of the arules package for more information on these parameters.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vwGlhQ7OCMeI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_d5tPn04COnz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "y1eWhst86hlD"
      }
    }
  ]
}